{"cells":[{"cell_type":"markdown","metadata":{"id":"6CLJruUpb9Mr"},"source":["# English-to-Spanish translation with a sequence-to-sequence Transformer\n","\n","**Author:** [fchollet](https://twitter.com/fchollet)<br>\n","**Date created:** 2021/05/26<br>\n","**Last modified:** 2023/08/17<br>\n","**Description:** Implementing a sequence-to-sequene Transformer and training it on a machine translation task."]},{"cell_type":"markdown","metadata":{"id":"KgXUbXgsb9Mv"},"source":["## Introduction\n","\n","In this example, we'll build a sequence-to-sequence Transformer model, which\n","we'll train on an English-to-Spanish machine translation task.\n","\n","You'll learn how to:\n","\n","- Vectorize text using the Keras `TextVectorization` layer.\n","- Implement a `TransformerEncoder` layer, a `TransformerDecoder` layer,\n","and a `PositionalEmbedding` layer.\n","- Prepare data for training a sequence-to-sequence model.\n","- Use the trained model to generate translations of never-seen-before\n","input sentences (sequence-to-sequence inference).\n","\n","The code featured here is adapted from the book\n","[Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition)\n","(chapter 11: Deep learning for text).\n","The present example is fairly barebones, so for detailed explanations of\n","how each building block works, as well as the theory behind Transformers,\n","I recommend reading the book."]},{"cell_type":"markdown","metadata":{"id":"6FvVefmjb9Mv"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lHYmsC5Ab9Mw"},"outputs":[],"source":["import pathlib\n","import random\n","import string\n","import re\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import TextVectorization"]},{"cell_type":"markdown","metadata":{"id":"wActp8nxb9Mx"},"source":["## Downloading the data\n","\n","We'll be working with an English-to-Spanish translation dataset\n","provided by [Anki](https://www.manythings.org/anki/). Let's download it:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":143,"status":"ok","timestamp":1700178751671,"user":{"displayName":"Vinod Rachapudi","userId":"01358194427146928226"},"user_tz":300},"id":"duq_mZiHiPhP","outputId":"570b055a-88b3-409f-d692-e6afc6a78b59"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\ntext_file = keras.utils.get_file(\\n    fname=\"spa-eng.zip\",\\n    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\\n    extract=True,\\n)\\ntext_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\"\\n'"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","text_file = keras.utils.get_file(\n","    fname=\"spa-eng.zip\",\n","    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n","    extract=True,\n",")\n","text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\"\n","'''"]},{"cell_type":"markdown","metadata":{"id":"mKZYy254b9My"},"source":["## Parsing the data\n","\n","Each line contains an English sentence and its corresponding Spanish sentence.\n","The English sentence is the *source sequence* and Spanish one is the *target sequence*.\n","We prepend the token `\"[start]\"` and we append the token `\"[end]\"` to the Spanish sentence."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6PVeelglojO4"},"outputs":[],"source":["text_file = keras.utils.get_file(\n","    fname=\"spa-eng.zip\",\n","    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\",\n","    #origin=\"https://www.manythings.org/anki/fra-eng.zip\",\n","    extract=True,\n",")\n","#text_file = pathlib.Path(text_file).parent / \"fra-eng\" / \"fra.txt\"\n","text_file = pathlib.Path(text_file).parent / \"fra.txt\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":140,"status":"ok","timestamp":1700178761029,"user":{"displayName":"Vinod Rachapudi","userId":"01358194427146928226"},"user_tz":300},"id":"EtymEarDZGcH","outputId":"e2c00cf6-a805-497d-eed3-c4040ceacedb"},"outputs":[{"name":"stdout","output_type":"stream","text":["/root/.keras/datasets\n","/root/.keras/datasets/fra.txt\n"]}],"source":["print(pathlib.Path(text_file).parent)\n","print(text_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jr_0jSKVb9My"},"outputs":[],"source":["with open(text_file) as f:\n","    lines = f.read().split(\"\\n\")[:-1]\n","text_pairs = []\n","for line in lines:\n","    eng, spa = line.split(\"\\t\")\n","    spa = \"[start] \" + spa + \" [end]\"\n","    text_pairs.append((eng, spa))"]},{"cell_type":"markdown","metadata":{"id":"QAtd0uokb9Mz"},"source":["Here's what our sentence pairs look like:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":135,"status":"ok","timestamp":1700178764173,"user":{"displayName":"Vinod Rachapudi","userId":"01358194427146928226"},"user_tz":300},"id":"l5OXSZGTb9Mz","outputId":"bc6a97c7-59db-4a5a-ef52-62b912fbc130"},"outputs":[{"name":"stdout","output_type":"stream","text":["(\"It's just a piece of paper.\", \"[start] Ce n'est qu'un bout de papier. [end]\")\n","('As I got the train this morning, I met an old friend of mine.', \"[start] En prenant le train ce matin, j'ai rencontré un vieil ami. [end]\")\n","(\"It's been three years since we got married.\", '[start] Ça fait trois ans que nous nous sommes mariés. [end]')\n","('Walls have ears.', '[start] Les murs ont des oreilles. [end]')\n","('No matter how sneaky you are, you can never surprise yourself.', '[start] Tout sournois que vous soyez, vous ne pouvez jamais vous surprendre vous-mêmes. [end]')\n"]}],"source":["for _ in range(5):\n","    print(random.choice(text_pairs))"]},{"cell_type":"markdown","metadata":{"id":"f0HMyJbeb9Mz"},"source":["Now, let's split the sentence pairs into a training set, a validation set,\n","and a test set."]},{"cell_type":"markdown","metadata":{"id":"c3d0i03scl_o"},"source":["# ** Changing to Train: 50%, Validation: 35% & Test: 15%**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"elapsed":134,"status":"ok","timestamp":1700178765908,"user":{"displayName":"Vinod Rachapudi","userId":"01358194427146928226"},"user_tz":300},"id":"vi1LlMmNb9Mz","outputId":"43977634-cdc0-492a-a1b7-d85650179ee7"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nrandom.shuffle(text_pairs)\\nnum_val_samples = int(0.15 * len(text_pairs))\\nnum_train_samples = len(text_pairs) - 2 * num_val_samples\\ntrain_pairs = text_pairs[:num_train_samples]\\nval_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\\ntest_pairs = text_pairs[num_train_samples + num_val_samples :]\\n\\nprint(f\"{len(text_pairs)} total pairs\")\\nprint(f\"{len(train_pairs)} training pairs\")\\nprint(f\"{len(val_pairs)} validation pairs\")\\nprint(f\"{len(test_pairs)} test pairs\")\\n'"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","random.shuffle(text_pairs)\n","num_val_samples = int(0.15 * len(text_pairs))\n","num_train_samples = len(text_pairs) - 2 * num_val_samples\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n","test_pairs = text_pairs[num_train_samples + num_val_samples :]\n","\n","print(f\"{len(text_pairs)} total pairs\")\n","print(f\"{len(train_pairs)} training pairs\")\n","print(f\"{len(val_pairs)} validation pairs\")\n","print(f\"{len(test_pairs)} test pairs\")\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":277,"status":"ok","timestamp":1700178768292,"user":{"displayName":"Vinod Rachapudi","userId":"01358194427146928226"},"user_tz":300},"id":"GKeRFX-9cjsV","outputId":"a2cd5a50-7380-4a78-8e2f-755545e8643a"},"outputs":[{"name":"stdout","output_type":"stream","text":["167130\n","83565\n","58495\n","25070\n","167130 total pairs\n","83565 training pairs\n","58495 validation pairs\n","25070 test pairs\n"]}],"source":["random.shuffle(text_pairs)\n","num_train_samples = int(0.50 * len(text_pairs))\n","num_val_samples = int(0.35 * len(text_pairs))\n","num_test_samples = len(text_pairs) - num_train_samples - num_val_samples\n","#num_train_samples = len(text_pairs) - 2 * num_val_samples\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n","test_pairs = text_pairs[num_train_samples + num_val_samples :]\n","\n","print(len(text_pairs))\n","print(num_train_samples)\n","print(num_val_samples)\n","print(num_test_samples)\n","\n","print(f\"{len(text_pairs)} total pairs\")\n","print(f\"{len(train_pairs)} training pairs\")\n","print(f\"{len(val_pairs)} validation pairs\")\n","print(f\"{len(test_pairs)} test pairs\")"]},{"cell_type":"markdown","metadata":{"id":"8a6EEwonb9M0"},"source":["## Vectorizing the text data\n","\n","We'll use two instances of the `TextVectorization` layer to vectorize the text\n","data (one for English and one for Spanish),\n","that is to say, to turn the original strings into integer sequences\n","where each integer represents the index of a word in a vocabulary.\n","\n","The English layer will use the default string standardization (strip punctuation characters)\n","and splitting scheme (split on whitespace), while\n","the Spanish layer will use a custom standardization, where we add the character\n","`\"¿\"` to the set of punctuation characters to be stripped.\n","\n","Note: in a production-grade machine translation model, I would not recommend\n","stripping the punctuation characters in either language. Instead, I would recommend turning\n","each punctuation character into its own token,\n","which you could achieve by providing a custom `split` function to the `TextVectorization` layer."]},{"cell_type":"markdown","metadata":{"id":"cKTnNxb3eWqt"},"source":["# ** Changing the vocab length to 2000 and sequence length to 70.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VvwD33MAb9M0"},"outputs":[],"source":["strip_chars = string.punctuation + \"¿\"\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n","\n","vocab_size = 2000   #15000\n","sequence_length = 70 #20\n","batch_size = 64\n","\n","\n","def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)\n","    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n","\n","\n","eng_vectorization = TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length,\n",")\n","spa_vectorization = TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length + 1,\n","    standardize=custom_standardization,\n",")\n","train_eng_texts = [pair[0] for pair in train_pairs]\n","train_spa_texts = [pair[1] for pair in train_pairs]\n","eng_vectorization.adapt(train_eng_texts)\n","spa_vectorization.adapt(train_spa_texts)"]},{"cell_type":"markdown","metadata":{"id":"mffUfFkKb9M0"},"source":["Next, we'll format our datasets.\n","\n","At each training step, the model will seek to predict target words N+1 (and beyond)\n","using the source sentence and the target words 0 to N.\n","\n","As such, the training dataset will yield a tuple `(inputs, targets)`, where:\n","\n","- `inputs` is a dictionary with the keys `encoder_inputs` and `decoder_inputs`.\n","`encoder_inputs` is the vectorized source sentence and `decoder_inputs` is the target sentence \"so far\",\n","that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target sentence.\n","- `target` is the target sentence offset by one step:\n","it provides the next words in the target sentence -- what the model will try to predict."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9bm3Zo1db9M0"},"outputs":[],"source":["\n","def format_dataset(eng, spa):\n","    eng = eng_vectorization(eng)\n","    spa = spa_vectorization(spa)\n","    return (\n","        {\n","            \"encoder_inputs\": eng,\n","            \"decoder_inputs\": spa[:, :-1],\n","        },\n","        spa[:, 1:],\n","    )\n","\n","\n","def make_dataset(pairs):\n","    eng_texts, spa_texts = zip(*pairs)\n","    eng_texts = list(eng_texts)\n","    spa_texts = list(spa_texts)\n","    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset)\n","    return dataset.shuffle(2048).prefetch(16).cache()\n","\n","\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)"]},{"cell_type":"markdown","metadata":{"id":"pLge2Hm0b9M0"},"source":["Let's take a quick look at the sequence shapes\n","(we have batches of 64 pairs, and all sequences are 20 steps long):"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1043,"status":"ok","timestamp":1700178806262,"user":{"displayName":"Vinod Rachapudi","userId":"01358194427146928226"},"user_tz":300},"id":"KiiiaFZwb9M0","outputId":"68f8d0e5-dd1d-4954-b2a6-ec2f98893719"},"outputs":[{"name":"stdout","output_type":"stream","text":["inputs[\"encoder_inputs\"].shape: (64, 70)\n","inputs[\"decoder_inputs\"].shape: (64, 70)\n","targets.shape: (64, 70)\n"]}],"source":["for inputs, targets in train_ds.take(1):\n","    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n","    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n","    print(f\"targets.shape: {targets.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"lhbY9lr-b9M1"},"source":["## Building the model\n","\n","Our sequence-to-sequence Transformer consists of a `TransformerEncoder`\n","and a `TransformerDecoder` chained together. To make the model aware of word order,\n","we also use a `PositionalEmbedding` layer.\n","\n","The source sequence will be pass to the `TransformerEncoder`,\n","which will produce a new representation of it.\n","This new representation will then be passed\n","to the `TransformerDecoder`, together with the target sequence so far (target words 0 to N).\n","The `TransformerDecoder` will then seek to predict the next words in the target sequence (N+1 and beyond).\n","\n","A key detail that makes this possible is causal masking\n","(`use_causal_mask=True` in the first attention layer of the `TransformerDecoder`).\n","The `TransformerDecoder` sees the entire sequences at once, and thus we must make\n","sure that it only uses information from target tokens 0 to N when predicting token N+1\n","(otherwise, it could use information from the future, which would\n","result in a model that cannot be used at inference time)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RnCmK9qMb9M1"},"outputs":[],"source":["\n","class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [\n","                layers.Dense(dense_dim, activation=\"relu\"),\n","                layers.Dense(embed_dim),\n","            ]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask=None):\n","        attention_output = self.attention(query=inputs, value=inputs, key=inputs)\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update(\n","            {\n","                \"embed_dim\": self.embed_dim,\n","                \"dense_dim\": self.dense_dim,\n","                \"num_heads\": self.num_heads,\n","            }\n","        )\n","        return config\n","\n","\n","class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=vocab_size, output_dim=embed_dim\n","        )\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=embed_dim\n","        )\n","        self.sequence_length = sequence_length\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update(\n","            {\n","                \"sequence_length\": self.sequence_length,\n","                \"vocab_size\": self.vocab_size,\n","                \"embed_dim\": self.embed_dim,\n","            }\n","        )\n","        return config\n","\n","\n","class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.latent_dim = latent_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [\n","                layers.Dense(latent_dim, activation=\"relu\"),\n","                layers.Dense(embed_dim),\n","            ]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.add = layers.Add()  # instead of `+` to preserve mask\n","        self.supports_masking = True\n","\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        attention_output_1 = self.attention_1(\n","            query=inputs, value=inputs, key=inputs, use_causal_mask=True\n","        )\n","        out_1 = self.layernorm_1(self.add([inputs, attention_output_1]))\n","\n","        attention_output_2 = self.attention_2(\n","            query=out_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","        )\n","        out_2 = self.layernorm_2(self.add([out_1, attention_output_2]))\n","\n","        proj_output = self.dense_proj(out_2)\n","        return self.layernorm_3(self.add([out_2, proj_output]))\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update(\n","            {\n","                \"embed_dim\": self.embed_dim,\n","                \"latent_dim\": self.latent_dim,\n","                \"num_heads\": self.num_heads,\n","            }\n","        )\n","        return config\n"]},{"cell_type":"markdown","metadata":{"id":"f3VS1-Nqb9M1"},"source":["Next, we assemble the end-to-end model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DxH-75TBb9M1"},"outputs":[],"source":["embed_dim = 256\n","latent_dim = 2048\n","num_heads = 8\n","\n","encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n","encoder = keras.Model(encoder_inputs, encoder_outputs)\n","\n","decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n","encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n","x = layers.Dropout(0.5)(x)\n","decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n","\n","decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n","transformer = keras.Model(\n","    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",")"]},{"cell_type":"markdown","metadata":{"id":"hjMYUR9Tb9M2"},"source":["## Training our model\n","\n","We'll use accuracy as a quick way to monitor training progress on the validation data.\n","Note that machine translation typically uses BLEU scores as well as other metrics, rather than accuracy.\n","\n","Here we only train for 1 epoch, but to get the model to actually converge\n","you should train for at least 30 epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"P5mlaiDwb9M2","outputId":"56bc1e56-9313-4500-df80-e4a610f87cfd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"transformer\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," encoder_inputs (InputLayer  [(None, None)]               0         []                            \n"," )                                                                                                \n","                                                                                                  \n"," positional_embedding_4 (Po  (None, None, 256)            529920    ['encoder_inputs[0][0]']      \n"," sitionalEmbedding)                                                                               \n","                                                                                                  \n"," decoder_inputs (InputLayer  [(None, None)]               0         []                            \n"," )                                                                                                \n","                                                                                                  \n"," transformer_encoder_2 (Tra  (None, None, 256)            3155456   ['positional_embedding_4[0][0]\n"," nsformerEncoder)                                                   ']                            \n","                                                                                                  \n"," model_5 (Functional)        (None, None, 2000)           6303440   ['decoder_inputs[0][0]',      \n","                                                                     'transformer_encoder_2[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n","==================================================================================================\n","Total params: 9988816 (38.10 MB)\n","Trainable params: 9988816 (38.10 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n","Epoch 1/20\n","1306/1306 [==============================] - 207s 154ms/step - loss: 2.5428 - accuracy: 0.5342 - val_loss: 1.6606 - val_accuracy: 0.6530\n","Epoch 2/20\n","1306/1306 [==============================] - 198s 152ms/step - loss: 1.6651 - accuracy: 0.6559 - val_loss: 1.4186 - val_accuracy: 0.6892\n","Epoch 3/20\n","1306/1306 [==============================] - 198s 152ms/step - loss: 1.4638 - accuracy: 0.6882 - val_loss: 1.3393 - val_accuracy: 0.7034\n","Epoch 4/20\n","1306/1306 [==============================] - 198s 152ms/step - loss: 1.3580 - accuracy: 0.7076 - val_loss: 1.3011 - val_accuracy: 0.7117\n","Epoch 5/20\n","1306/1306 [==============================] - 198s 152ms/step - loss: 1.2832 - accuracy: 0.7221 - val_loss: 1.2796 - val_accuracy: 0.7175\n","Epoch 6/20\n","1306/1306 [==============================] - 198s 151ms/step - loss: 1.2265 - accuracy: 0.7335 - val_loss: 1.2796 - val_accuracy: 0.7196\n","Epoch 7/20\n","1306/1306 [==============================] - 197s 151ms/step - loss: 1.1789 - accuracy: 0.7436 - val_loss: 1.2811 - val_accuracy: 0.7234\n","Epoch 8/20\n","1306/1306 [==============================] - 198s 151ms/step - loss: 1.1353 - accuracy: 0.7522 - val_loss: 1.2857 - val_accuracy: 0.7254\n","Epoch 9/20\n","1306/1306 [==============================] - 198s 151ms/step - loss: 1.0995 - accuracy: 0.7595 - val_loss: 1.2853 - val_accuracy: 0.7273\n","Epoch 10/20\n","1306/1306 [==============================] - 197s 151ms/step - loss: 1.0679 - accuracy: 0.7667 - val_loss: 1.2953 - val_accuracy: 0.7282\n","Epoch 11/20\n","1306/1306 [==============================] - 197s 151ms/step - loss: 1.0372 - accuracy: 0.7728 - val_loss: 1.3032 - val_accuracy: 0.7298\n","Epoch 12/20\n","1306/1306 [==============================] - 196s 150ms/step - loss: 1.0084 - accuracy: 0.7786 - val_loss: 1.3092 - val_accuracy: 0.7314\n","Epoch 13/20\n","1306/1306 [==============================] - 196s 150ms/step - loss: 0.9834 - accuracy: 0.7841 - val_loss: 1.3258 - val_accuracy: 0.7320\n","Epoch 14/20\n","1306/1306 [==============================] - 196s 150ms/step - loss: 0.9609 - accuracy: 0.7884 - val_loss: 1.3270 - val_accuracy: 0.7330\n","Epoch 15/20\n","1306/1306 [==============================] - 236s 181ms/step - loss: 0.9395 - accuracy: 0.7927 - val_loss: 1.3485 - val_accuracy: 0.7312\n","Epoch 16/20\n","1306/1306 [==============================] - 197s 151ms/step - loss: 0.9189 - accuracy: 0.7967 - val_loss: 1.3635 - val_accuracy: 0.7330\n","Epoch 17/20\n","1306/1306 [==============================] - 196s 150ms/step - loss: 0.9005 - accuracy: 0.8007 - val_loss: 1.3664 - val_accuracy: 0.7328\n","Epoch 18/20\n","1306/1306 [==============================] - 196s 150ms/step - loss: 0.8835 - accuracy: 0.8041 - val_loss: 1.3615 - val_accuracy: 0.7337\n","Epoch 19/20\n","1306/1306 [==============================] - 235s 180ms/step - loss: 0.8673 - accuracy: 0.8078 - val_loss: 1.3685 - val_accuracy: 0.7355\n","Epoch 20/20\n","1306/1306 [==============================] - 197s 151ms/step - loss: 0.8526 - accuracy: 0.8105 - val_loss: 1.3968 - val_accuracy: 0.7333\n"]}],"source":["epochs = 20  # This should be at least 30 for convergence\n","\n","transformer.summary()\n","transformer.compile(\n","    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",")\n","history = transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"]},{"cell_type":"code","source":["import cv2\n","from google.colab import drive\n","\n","drive.mount(\"/content/gdrive\")\n","transformer.save('/content/gdrive/MyDrive/ColabNotebooks/assign10_a.h5')\n","\n","#img = cv2.imread('/content/gdrive/MyDrive/ColabNotebooks/sizedgrumpycat.jpg', cv2.IMREAD_GRAYSCALE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-9oo9ECwC4fF","executionInfo":{"status":"ok","timestamp":1700187105795,"user_tz":300,"elapsed":21940,"user":{"displayName":"Vinod Rachapudi","userId":"01358194427146928226"}},"outputId":"a8a68822-f07d-45f8-aac3-d814c5b96524"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZAZAEj2Fb9M2"},"source":["## Decoding test sentences\n","\n","Finally, let's demonstrate how to translate brand new English sentences.\n","We simply feed into the model the vectorized English sentence\n","as well as the target token `\"[start]\"`, then we repeatedly generated the next token, until\n","we hit the token `\"[end]\"`."]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14769,"status":"ok","timestamp":1700189724548,"user":{"displayName":"Vinod Rachapudi","userId":"01358194427146928226"},"user_tz":300},"id":"NgotWHT2b9M2","outputId":"f0295f61-449d-4b87-e6a7-5ede7821ff1f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Over-sleeping is no excuse for being late.\n","[start] [UNK] nest pas [UNK] en retard [end]\n","I left my card at home.\n","[start] jai laissé ma carte [UNK] chez moi [end]\n","I'm not so sure.\n","[start] je nen suis pas si sûr [end]\n","You should get yourself examined by the doctor immediately.\n","[start] tu devrais [UNK] [UNK] le médecin immédiatement [end]\n","Where is the dog?\n","[start] où est le chien [end]\n","I'd like to live in that house.\n","[start] jaimerais vivre dans cette maison [end]\n","We left the final decision to him.\n","[start] nous avons laissé la décision lui [UNK] [end]\n","I hope you're well.\n","[start] jespère que vous êtes bien [end]\n","What are we having for dinner tonight?\n","[start] [UNK] à dîner ce soir [end]\n","She has never visited him.\n","[start] elle na jamais [UNK] [UNK] [end]\n","You were hot, weren't you?\n","[start] tu étais chaud nestce pas [end]\n","He didn't answer the phone, so I sent him an email.\n","[start] il na pas répondu au téléphone alors je lui ai envoyé un [UNK] [end]\n","I'm as tall as Tom.\n","[start] je suis aussi grand que tom [end]\n","How could anyone be so stupid?\n","[start] comment quiconque pourrait être si stupide [end]\n","You're the same age as my girlfriend.\n","[start] vous avez le même âge que ma petite amie [end]\n","Did you know him well?\n","[start] le [UNK] bien [end]\n","How many students are there in total?\n","[start] combien [UNK] y atil en [UNK] [end]\n","I like the way you look.\n","[start] jaime la façon dont vous avez lair [end]\n","I like football.\n","[start] jaime le [UNK] [end]\n","He made the children laugh.\n","[start] il a fait rire de rire [end]\n","We'll make the right decision.\n","[start] nous [UNK] la bonne décision [end]\n","Right now they're all sleeping.\n","[start] en ce moment elles [UNK] toutes [end]\n","How would you describe yourself?\n","[start] comment vous [UNK] [end]\n","I think you'd be interested in this.\n","[start] je pense que vous seriez [UNK] par cela [end]\n","We missed the deadline.\n","[start] nous avons [UNK] le [UNK] [end]\n","He came at about four o'clock.\n","[start] il est venu à [UNK] quatre heures [end]\n","You're the most important person in my life.\n","[start] vous êtes la plus importante en personne dans ma vie [end]\n","Are you having fun?\n","[start] Êtesvous en train de [UNK] [end]\n","I don't expect you'd remember.\n","[start] je ne [UNK] pas à ce que tu [UNK] [end]\n","How much is it?\n","[start] combien cela [UNK] [end]\n"]}],"source":["spa_vocab = spa_vectorization.get_vocabulary()\n","spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n","max_decoded_sentence_length = 20\n","\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = eng_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n","        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n","\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = spa_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","\n","        if sampled_token == \"[end]\":\n","            break\n","    return decoded_sentence\n","\n","\n","test_eng_texts = [pair[0] for pair in test_pairs]\n","for _ in range(30):\n","    input_sentence = random.choice(test_eng_texts)\n","    translated = decode_sequence(input_sentence)\n","    print(input_sentence)\n","    print(translated)"]},{"cell_type":"markdown","metadata":{"id":"U4wSUvasb9M2"},"source":["After 30 epochs, we get results such as:\n","\n","> She handed him the money.\n","> [start] ella le pasó el dinero [end]\n","\n","> Tom has never heard Mary sing.\n","> [start] tom nunca ha oído cantar a mary [end]\n","\n","> Perhaps she will come tomorrow.\n","> [start] tal vez ella vendrá mañana [end]\n","\n","> I love to write.\n","> [start] me encanta escribir [end]\n","\n","> His French is improving little by little.\n","> [start] su francés va a [UNK] sólo un poco [end]\n","\n","> My hotel told me to call you.\n","> [start] mi hotel me dijo que te [UNK] [end]"]},{"cell_type":"markdown","metadata":{"id":"t7v3oSfQe2l6"},"source":["# **Bleu score**"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":159,"status":"ok","timestamp":1700189739063,"user":{"displayName":"Vinod Rachapudi","userId":"01358194427146928226"},"user_tz":300},"id":"Ybs20Ppxe189","outputId":"09b5cd09-e90a-4fd6-dc01-5929931915e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["BLEU score -> 1.2213386697554703e-77\n","BLEU score -> 1.0270193092081295e-77\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"]}],"source":["from nltk.translate.bleu_score import sentence_bleu\n","reference = [\n","    'this is a dog'.split(),\n","    'it is dog'.split(),\n","    'dog it is'.split(),\n","    'a dog, it is'.split()\n","]\n","candidate = 'it is dog'.split()\n","print('BLEU score -> {}'.format(sentence_bleu(reference, candidate )))\n","\n","candidate = 'it is a dog'.split()\n","print('BLEU score -> {}'.format(sentence_bleu(reference, candidate)))"]}],"metadata":{"accelerator":"GPU","colab":{"toc_visible":true,"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":0}